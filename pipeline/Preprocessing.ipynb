{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on:\n",
        "- https://www.kaggle.com/ashishpatel26/preprocessing-lightgbm-xgboost\n",
        "- https://github.com/Shitao-zz/Kaggle-House-Prices-Advanced-Regression-Techniques\n",
        "- https://www.kaggle.com/vinicius150987/regression-boston-sklearn-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625471033251
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#import swifter\n",
        "\n",
        "from category_encoders import HelmertEncoder, SumEncoder, OneHotEncoder\n",
        "#from fancyimpute import KNN, SoftImpute, BiScaler, IterativeSVD, MatrixFactorization, SimpleFill\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_boston, make_classification, make_regression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
        "from sklearn.linear_model import BayesianRidge, SGDRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error, median_absolute_error\n",
        "\n",
        "pd.options.mode.chained_assignment = None \n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471189902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cat_cols = ['TODO']\n",
        "num_cols = ['TODO']\n",
        "target = ['label']\n",
        "len(cat_cols) + len(num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471194235
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "encode_cat = False\n",
        "scaling = True\n",
        "\n",
        "save_data = False\n",
        "#drop_nan_rows = False\n",
        "#drop_error_data = False\n",
        "\n",
        "random_state = 42\n",
        "\n",
        "#imputer_num = IterativeImputer(estimator=SGDRegressor(loss='huber', penalty='l1'), verbose=2, max_iter=1)\n",
        "#imputer_num = IterativeImputer(estimator=BayesianRidge(), verbose=2, max_iter=30)\n",
        "#imputer_num = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, criterion=\"mae\", n_jobs=-1, random_state=random_state), verbose=2, max_iter=5, n_nearest_features=3)\n",
        "#imputer_num = IterativeImputer(estimator=KNeighborsRegressor(n_jobs=-1), n_nearest_features=5, verbose=2, max_iter=5)\n",
        "imputer_num = SimpleImputer()\n",
        "#imputer_num = KNNImputer(n_neighbors=5) # also slow https://github.com/scikit-learn/scikit-learn/issues/18186, 33min for 200k rows TODO: normalize before! but gives error :(\n",
        "\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "encoder = OneHotEncoder(cat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471196468
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def save_memory(df, compress_float=True):\n",
        "    features = df.columns\n",
        "    for i in range( df.shape[1] ):\n",
        "        if df.dtypes[i] == 'uint8':\n",
        "            df[features[i]] = df[features[i]].astype( np.int8 )\n",
        "        elif df.dtypes[i] == 'bool':\n",
        "            df[features[i]] = df[features[i]].astype( np.int8 )\n",
        "        elif df.dtypes[i] == 'uint32':\n",
        "            df[features[i]] = df[features[i]].astype( np.int32 )\n",
        "        elif df.dtypes[i] == 'int64':\n",
        "            df[features[i]] = df[features[i]].astype( np.int32 )\n",
        "        elif df.dtypes[i] == 'float64' and compress_float:\n",
        "            df[features[i]] = df[features[i]].astype( np.float32 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471119859
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/data_original.csv')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471203221
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# remove inf values\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "for cat in cat_cols:\n",
        "    df[cat] = df[cat].astype(\"category\")\n",
        "\n",
        "# replace string nan \n",
        "df = df.replace('nan', np.nan)\n",
        "\n",
        "df = df[target + cat_cols + num_cols]\n",
        "\n",
        "print(df.shape)\n",
        "print(df.dtypes)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471291810
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('data/data_test.csv')\n",
        "\n",
        "# remove inf values\n",
        "df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "for cat in cat_cols:\n",
        "    df_test[cat] = df_test[cat].astype(\"category\")\n",
        "\n",
        "# replace string nan \n",
        "df_test = df_test.replace('nan', np.nan)\n",
        "\n",
        "X_test = df_test[['id'] + cat_cols + num_cols]\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_test.dtypes)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471488318
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "X = df.drop(target, axis=1)\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
        "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471761222
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Imputation\n",
        "X_train[num_cols] = imputer_num.fit_transform(X_train[num_cols])\n",
        "X_valid[num_cols] = imputer_num.transform(X_valid[num_cols])\n",
        "X_test[num_cols] = imputer_num.transform(X_test[num_cols])\n",
        "\n",
        "X_train[cat_cols] = imputer_cat.fit_transform(X_train[cat_cols])\n",
        "X_valid[cat_cols] = imputer_cat.transform(X_valid[cat_cols])\n",
        "X_test[cat_cols] = imputer_cat.transform(X_test[cat_cols])\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471765319
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# NORMALIZE/STANDARDIZE\n",
        "if scaling:\n",
        "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "    X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471765516
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# One Hot Encoding\n",
        "if encode_cat:\n",
        "    X_train = encoder.fit_transform(X_train, y_train)\n",
        "    X_valid = encoder.transform(X_valid)\n",
        "    X_test = encoder.transform(X_test)\n",
        "else:\n",
        "    for cat in cat_cols:\n",
        "        X_train[cat] = X_train[cat].astype(\"category\")\n",
        "        X_valid[cat] = X_valid[cat].astype(\"category\")\n",
        "        X_test[cat] = X_test[cat].astype(\"category\")\n",
        "        print(f\"{cat} {X_test[cat].dtype}\")\n",
        "y_train = y_train.astype(\"float32\")\n",
        "y_valid = y_valid.astype(\"float32\")\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625471765713
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "X_test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625472690294
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "model = LGBMRegressor(n_jobs=-1, objective=\"regression_l1\",\n",
        "            boosting_type='dart',\n",
        "            learning_rate=0.13110563910426518,\n",
        "            max_depth=50,\n",
        "            n_estimators=1000,\n",
        "            num_leaves=200,\n",
        "            reg_alpha=1.5921278701590583e-05,\n",
        "            reg_lambda=1.0,\n",
        "            subsample_for_bin=200000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train MAE: \", mean_absolute_error(y_train, np.abs(model.predict(X_train))))\n",
        "print(\"MeanAE: \", mean_absolute_error(y_valid, np.abs(model.predict(X_valid))))\n",
        "print(\"MedianAE: \", median_absolute_error(y_valid, np.abs(model.predict(X_valid))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625472692156
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#raise SystemExit(\"Stop right there!\")\n",
        "np.abs(model.predict(X_test.drop('DW_PET_VST_ID', axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625229101656
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "save_memory(X_train, compress_float=False)\n",
        "save_memory(X_test, compress_float=False)\n",
        "print(X_train.dtypes)\n",
        "\n",
        "if encode_cat:\n",
        "    file_name = \"_pre_simple.csv\"\n",
        "else:\n",
        "    file_name = \"_pre_simple_no_enc.csv\"\n",
        "\n",
        "if save_data:\n",
        "    X_train.to_csv('data/X_train' + file_name, index=False)\n",
        "    X_valid.to_csv('data/X_valid' + file_name, index=False)\n",
        "    X_test.to_csv('data/X_test' + file_name, index=False)\n",
        "    y_train.to_csv('data/y_train' + file_name, index=False)\n",
        "    y_valid.to_csv('data/y_valid' + file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625229112409
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "flist = num_cols\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_valid = X_valid.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "# perform a polynomial features transform of the dataset\n",
        "trans = PolynomialFeatures(degree=2) # exponential! only 2 or 3\n",
        "trans_train = trans.fit_transform(X_train[flist])\n",
        "trans_valid = trans.transform(X_valid[flist])\n",
        "trans_test = trans.transform(X_test[flist])\n",
        "\n",
        "X_train = pd.concat([X_train, pd.DataFrame(trans_train, columns=trans.get_feature_names(X_train[flist].columns))], axis=1)\n",
        "X_valid = pd.concat([X_valid, pd.DataFrame(trans_valid, columns=trans.get_feature_names(X_valid[flist].columns))], axis=1)\n",
        "X_test = pd.concat([X_test, pd.DataFrame(trans_test, columns=trans.get_feature_names(X_test[flist].columns))], axis=1)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625229112534
        }
      },
      "outputs": [],
      "source": [
        "def kmeans(X_tr, X_vl, X_te, flist):\n",
        "    flist_kmeans = []\n",
        "    for ncl in range(2,11):\n",
        "        cls = KMeans(n_clusters=ncl)\n",
        "        cls.fit_predict(X_train[flist].values)\n",
        "        X_tr['kmeans_cluster_'+str(ncl)] = cls.predict(X_tr[flist].values)\n",
        "        X_vl['kmeans_cluster_'+str(ncl)] = cls.predict(X_vl[flist].values)\n",
        "        X_te['kmeans_cluster_'+str(ncl)] = cls.predict(X_te[flist].values)\n",
        "        flist_kmeans.append('kmeans_cluster_'+str(ncl))\n",
        "    print(flist_kmeans)\n",
        "    \n",
        "    return X_tr, X_vl, X_te"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625229744445
        }
      },
      "outputs": [],
      "source": [
        "flist = num_cols\n",
        "X_train, X_valid, X_test = kmeans(X_train, X_valid, X_test, flist)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625229801622
        }
      },
      "outputs": [],
      "source": [
        "N_COMP = 20           ### Number of decomposition components ###\n",
        "flist = num_cols\n",
        "\n",
        "print(\"\\nStart decomposition process...\")\n",
        "print(\"PCA\")\n",
        "pca = PCA(n_components=N_COMP, random_state=random_state)\n",
        "pca_results_train = pca.fit_transform(X_train[flist].values)\n",
        "pca_results_valid = pca.transform(X_valid[flist].values)\n",
        "pca_results_test = pca.transform(X_test[flist].values)\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "print(\"tSVD\")\n",
        "tsvd = TruncatedSVD(n_components=N_COMP, random_state=random_state)\n",
        "tsvd_results_train = tsvd.fit_transform(X_train[flist].values)\n",
        "tsvd_results_valid = tsvd.transform(X_valid[flist].values)\n",
        "tsvd_results_test = tsvd.transform(X_test[flist].values)\n",
        "\n",
        "print(\"ICA\")\n",
        "ica = FastICA(n_components=N_COMP, random_state=random_state)\n",
        "ica_results_train = ica.fit_transform(X_train[flist].values)\n",
        "ica_results_valid = ica.transform(X_valid[flist].values)\n",
        "ica_results_test = ica.transform(X_test[flist].values)\n",
        "\n",
        "print(\"GRP\")\n",
        "grp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=random_state)\n",
        "grp_results_train = grp.fit_transform(X_train[flist].values)\n",
        "grp_results_valid = grp.transform(X_valid[flist].values)\n",
        "grp_results_test = grp.transform(X_test[flist].values)\n",
        "\n",
        "print(\"SRP\")\n",
        "srp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=random_state)\n",
        "srp_results_train = srp.fit_transform(X_train[flist].values)\n",
        "srp_results_valid = srp.transform(X_valid[flist].values)\n",
        "srp_results_test = srp.transform(X_test[flist].values)\n",
        "\n",
        "print(\"Append decomposition components to datasets...\")\n",
        "for i in range(1, N_COMP + 1):\n",
        "    X_train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
        "    X_valid['pca_' + str(i)] = pca_results_valid[:, i - 1]\n",
        "    X_test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
        "    \n",
        "    X_train['ica_' + str(i)] = ica_results_train[:, i - 1]\n",
        "    X_valid['ica_' + str(i)] = ica_results_valid[:, i - 1]\n",
        "    X_test['ica_' + str(i)] = ica_results_test[:, i - 1]\n",
        "\n",
        "    X_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
        "    X_valid['tsvd_' + str(i)] = tsvd_results_valid[:, i - 1]\n",
        "    X_test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
        "\n",
        "    X_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
        "    X_valid['grp_' + str(i)] = grp_results_valid[:, i - 1]\n",
        "    X_test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
        "\n",
        "    X_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
        "    X_valid['srp_' + str(i)] = srp_results_valid[:, i - 1]\n",
        "    X_test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
        "    \n",
        "print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625230727997
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "model = LGBMRegressor(n_jobs=-1, objective=\"regression_l1\",\n",
        "            boosting_type='dart',\n",
        "            learning_rate=0.13110563910426518,\n",
        "            max_depth=50,\n",
        "            n_estimators=1000,\n",
        "            num_leaves=200,\n",
        "            reg_alpha=1.5921278701590583e-05,\n",
        "            reg_lambda=1.0,\n",
        "            subsample_for_bin=200000\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train MAE: \", mean_absolute_error(y_train, model.predict(X_train).abs()))\n",
        "print(\"MeanAE: \", mean_absolute_error(y_valid, model.predict(X_valid).abs()))\n",
        "print(\"MedianAE: \", median_absolute_error(y_valid, model.predict(X_valid).abs()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625230734472
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sub_df = pd.DataFrame(X_test['id'])\n",
        "sub_df['AgeEstimate'] = model.predict(X_test.drop('id', axis=1)).abs()\n",
        "sub_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1625230744634
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sub_df.to_csv(\"data/submission.csv\", index=False)\n",
        "sub_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1625231523024
        }
      },
      "outputs": [],
      "source": [
        "save_memory(X_train)\n",
        "save_memory(X_test)\n",
        "\n",
        "if save_data:\n",
        "    X_train.to_csv('data/X_train.csv', index=False)\n",
        "    X_valid.to_csv('data/X_valid.csv', index=False)\n",
        "    X_test.to_csv('data/X_test.csv', index=False)\n",
        "    y_train.to_csv('data/y_train.csv', index=False)\n",
        "    y_valid.to_csv('data/y_valid.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
